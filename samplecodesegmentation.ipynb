{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T13:30:21.745560Z",
     "iopub.status.busy": "2025-03-18T13:30:21.745233Z",
     "iopub.status.idle": "2025-03-18T14:00:00.249950Z",
     "shell.execute_reply": "2025-03-18T14:00:00.249057Z",
     "shell.execute_reply.started": "2025-03-18T13:30:21.745533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4800, Validation size: 1200\n",
      "Total parameters: 23907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 29/150 [01:59<08:18,  4.12s/batch, Batch Loss=0.7940, Mean Accuracy=0.5855, Mean IoU=0.4394]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    175\u001b[39m best_model_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     epoch_loss_train, mAcc_train, mIoU_train = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     epoch_loss_val, mAcc_val, mIoU_val = evaluate(model, val_dataloader, criterion, device, classes)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device, num_classes)\u001b[39m\n\u001b[32m     66\u001b[39m iou_metric = MulticlassJaccardIndex(num_classes=num_classes).to(device)\n\u001b[32m     67\u001b[39m pbar = tqdm(dataloader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m, unit=\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m       \u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\XuLyAnh\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\XuLyAnh\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\XuLyAnh\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\XuLyAnh\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\XuLyAnh\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mSemanticSegmentationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\u001b[32m     38\u001b[39m     label = cv2.imread(\u001b[38;5;28mself\u001b[39m.label_paths[idx])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import MulticlassJaccardIndex, MulticlassAccuracy\n",
    "import copy\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted([os.path.join(image_dir, img) for img in os.listdir(image_dir)])\n",
    "        self.label_paths = sorted([os.path.join(label_dir, lbl) for lbl in os.listdir(label_dir)])\n",
    "        self.class_colors = {\n",
    "            (2, 0, 0): 0,       \n",
    "            (127, 0, 0): 1,     \n",
    "            (248, 163, 191): 2  \n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label = cv2.imread(self.label_paths[idx])\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label_mask = np.zeros(label.shape[:2], dtype=np.uint8)\n",
    "        for rgb, idx in self.class_colors.items():\n",
    "            label_mask[np.all(label == rgb, axis=-1)] = idx\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label_mask = torch.from_numpy(label_mask).long()\n",
    "\n",
    "        return image, label_mask\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = SemanticSegmentationDataset(\n",
    "    image_dir='input',\n",
    "    label_dir='label',\n",
    "    transform=train_transform)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, num_classes):\n",
    "    model.train()\n",
    "    running_loss = 0.0  \n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes).to(device)\n",
    "    iou_metric = MulticlassJaccardIndex(num_classes=num_classes).to(device)\n",
    "    pbar = tqdm(dataloader, desc='Training', unit='batch')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)    \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        running_loss += loss.item() * images.size(0)      \n",
    "        preds = torch.argmax(outputs, dim=1)     \n",
    "        accuracy_metric(preds, labels)\n",
    "        iou_metric(preds, labels)\n",
    "        pbar.set_postfix({\n",
    "            'Batch Loss': f'{loss.item():.4f}',\n",
    "            'Mean Accuracy': f'{accuracy_metric.compute():.4f}',\n",
    "            'Mean IoU': f'{iou_metric.compute():.4f}',\n",
    "        }) \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)  \n",
    "    mean_accuracy = accuracy_metric.compute().cpu().numpy()\n",
    "    mean_iou = iou_metric.compute().cpu().numpy()\n",
    "   \n",
    "    return epoch_loss, mean_accuracy, mean_iou\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0    \n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=num_classes).to(device)\n",
    "    iou_metric = MulticlassJaccardIndex(num_classes=num_classes).to(device)\n",
    "    pbar = tqdm(dataloader, desc='Evaluating', unit='batch')\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # Update metrics\n",
    "            accuracy_metric(preds, labels)\n",
    "            iou_metric(preds, labels)\n",
    "            # Update tqdm description with metrics\n",
    "            pbar.set_postfix({\n",
    "                'Batch Loss': f'{loss.item():.4f}',\n",
    "                'Mean Accuracy': f'{accuracy_metric.compute():.4f}',\n",
    "                'Mean IoU': f'{iou_metric.compute():.4f}',\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    mean_accuracy = accuracy_metric.compute().cpu().numpy()\n",
    "    mean_iou = iou_metric.compute().cpu().numpy()\n",
    "    \n",
    "    return epoch_loss, mean_accuracy, mean_iou\n",
    "    \n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(myModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.upSampling = nn.Upsample(scale_factor=4, mode=\"bilinear\") \n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.conv_out = nn.Conv2d(64, n_classes, kernel_size=1, padding=0, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.upSampling(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)  \n",
    "val_size = total_size - train_size  \n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "classes = 3  \n",
    "model = myModel(classes)\n",
    "def count_parameters(model):  \n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "epoch_saved = 0\n",
    "best_val_mAcc = 0.0  \n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_train, mAcc_train, mIoU_train = train_epoch(model, train_dataloader, criterion, optimizer, device, classes)\n",
    "    epoch_loss_val, mAcc_val, mIoU_val = evaluate(model, val_dataloader, criterion, device, classes)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {epoch_loss_train:.4f}, Mean Accuracy: {mAcc_train:.4f}, Mean IoU: {mIoU_train:.4f}\")\n",
    "    print(f\"Validation Loss: {epoch_loss_val:.4f}, Mean Accuracy: {mAcc_val:.4f}, Mean IoU: {mIoU_val:.4f}\")\n",
    "\n",
    "    if mAcc_val >= best_val_mAcc:\n",
    "        epoch_saved = epoch + 1 \n",
    "        best_val_mAcc = mAcc_val\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "print(\"===================\")\n",
    "print(f\"Best Model at epoch : {epoch_saved}\")\n",
    "model.load_state_dict(best_model_state)\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model = model.module\n",
    "model_save = torch.jit.script(model)\n",
    "model_save.save(\"NguyenVanA_21119375.pt\")\n",
    "# Check again\n",
    "model = torch.jit.load(\"NguyenVanA_21119375.pt\")\n",
    "epoch_loss_val, mAcc_val, mIoU_val = evaluate(model, val_dataloader, criterion, device, classes)\n",
    "print(f\"Validation Loss: {epoch_loss_val:.4f}, Mean Accuracy: {mAcc_val:.4f}, Mean IoU: {mIoU_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6875502,
     "sourceId": 11038400,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
